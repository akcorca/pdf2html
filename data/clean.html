<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Converted PDF</title>
</head>
<body>
<h1>CleanAgent: Automating Data Standardization with LLM-based Agents</h1>
<p>Simon Fraser University</p>
<p>{dqi, zhengjie, jnwang}@sfu.ca</p>
<h2>ABSTRACT</h2>
<p>Data standardization is a crucial part of the data science life cycle. While tools like Pandas offer robust functionalities, their complex-ity and the manual effort required for customizing code to diverse column types pose significant challenges. Although large language models (LLMs) like ChatGPT have shown promise in automating this process through natural language understanding and code gen-eration, it still demands expert-level programming knowledge and continuous interaction for prompt refinement. To solve these chal-lenges, our key idea is to propose a Python library with declarative, unified APIs for standardizing different column types, simplify-ing the LLMâ€™s code generation with concise API calls. We first propose Dataprep.Clean , a component of the Dataprep Python Library, significantly reduces the coding complexity by enabling the standardization of specific column types with a single line of code. Then, we introduce the CleanAgent framework integrat-ing Dataprep.Clean and LLM-based agents to automate the data standardization process. With CleanAgent , data scientists only need to provide their requirements once, allowing for a hands-free process.</p>
<h2>1 INTRODUCTION</h2>
<p>Example 2. Still considering the data standardization task in Figure 1. For standardizing â€œAdmission Dateâ€ and â€œAddressâ€, data sci-entists need to write the datetime standardization code for â€œAdmission Dateâ€ and address standardization code for â€œAddressâ€ using regex. An example standardization code for â€œAddressâ€ is shown as follows .</p>
<pre><code>1 def standardize_address ( addr ):
2 # Extract street number and street name
3 street = pd . Series ( addr ). str . extract (r ' (\ d+ [^ ,]+) ' ).
squeeze ()
4 # Extract state name
5 state = " LA "
6 # Extract zipcode
7 zipcode = pd . Series ( addr ). str . extract (r ' (\ d {5}) ' ).
squeeze ()
8 # Output standardized address
9 return f"{ street }, { state }, { zipcode }"</code></pre>
<p>Data standardization, which is pivotal in the realm of data science,</p>
<p>aims to transform heterogeneous data formats within a single col-</p>
<p>umn into a unified data format. This crucial data preprocessing step</p>
<p>is essential for enabling effective data integration, data analysis,</p>
<p>and decision-making.</p>
<p>Example 1. We illustrate the data standardization task in Figure 1.</p>
<p>Given the input table ğ‘‡ , it is obvious that data in the â€œAdmission</p>
<p>Dateâ€ column and the â€œAddressâ€ column are in different formats, and the data in the cells of the â€œAdmission Dateâ€ column includes two different date formats. The goal of data standardization is to unify the data format in each column in ğ‘‡ , to get the standardized table</p>
<p>If the input table ğ‘‡ has other column types such as email and IP addresses, data scientists also need to write standardization code tailored for the new types, which is time-consuming.</p>
<p>ğ‘‡ satisfying the data scientistâ€™s requirements. In Figure 1, the data</p>
<p>Recently, the emerging LLMs have shown the potential to rev-</p>
<p>scientist inputs their requirement to standardize â€œAdmission Dateâ€</p>
<p>olutionize this process. By leveraging their natural language un-</p>
<p>with the â€œMM/DD/YYYY HH:MM:SSâ€ format. In the resulting ğ‘‡ , data in the cells of the â€œAdmission Dateâ€ column follows only one date format, i.e., the â€œMM/DD/YYYY HH:MM:SSâ€ format.</p>
<p>derstanding and code generation ability, these models could signif-icantly aid data scientists by autonomously generating standard-ization code in response to conversational prompts. However, this</p>
<p>Previously, data scientists heavily relied on libraries such as</p>
<p>method still necessitates detailed prompt crafting and often in-</p>
<p>Pandas [ 3 ] for data standardization tasks. Even though Pandas</p>
<p>volves multi-turn dialogues [ 1 ] for different column types in the</p>
<p>is a powerful tool, achieving data standardization often requires writing hundreds or thousands of lines of code. The standardization process for a single column involves identifying the column type, applying intricate methods such as regular expressions to each cell for validation, and converting each cell into desired formats.</p>
<p>table one by one, which limits the efficiency and practicality of adopting LLMs in the standardization process.</p>
<p>To overcome these limitations, our key idea is to introduce a Python library involving declarative and unified APIs specifically designed for standardizing different column types. This idea lowers</p>
<p>Moreover, a table may contain multiple columns, each possibly of a different type, requiring bespoke standardization code for each</p>
<p>the burden of the LLM, as it now only needs to convert natural lan-guage (NL) instructions into succinct, declarative API calls instead</p>
<p>column type.</p>
<p>of lengthy, procedural code. Such an approach simplifies the LLMâ€™s</p>
<p>code generation process for data standardization, requiring just a few lines of code.</p>
<h2>2 TYPE-SPECIFIC STANDARDIZATION API DESIGN</h2>
<p>The pursuit of simplicity, however, introduces two primary chal-</p>
<p>In this section, we first describe the common steps of data stan-</p>
<p>lenges. The first challenge ( C1 ) is the design of the declarative and</p>
<p>dardization. Then, we introduce the type-specific API design of</p>
<p>unified APIs for data standardization, ensuring it can effectively re-</p>
<p>Dataprep.Clean .</p>
<p>duce the intricacies involved in standardizing specific column types</p>
<p>Common Steps of Data Standardization.</p>
<p>Inspired by the steps of</p>
<p>(ideally one line of code per column type). The second challenge</p>
<p>how human users standardize data cells, we identify three common</p>
<p>( C2 ) centers on optimizing the interaction between data scientists</p>
<p>steps of data standardization. We take the</p>
<p>and LLMs. Our goal is to minimize human involvement, ideally allowing data scientists to input their standardization requirements</p>
<p>datetime column type</p>
<p>as an example to illustrate these steps.</p>
<p>Assume a data scientist is dealing with an</p>
<p>in one instance, thereby enabling an autonomous and hands-off data standardization process.</p>
<p>datetime column</p>
<p>including two records "Thu Sep 25 10:36:28 2003" and "1996.07.10 AD at 15:08:56" . The data scientist wants to unify the messy column</p>
<p>To solve C1 , we propose the type-specific Clean module in the</p>
<p>into a target format "YYYY-MM-DD hh:mm:ss" .</p>
<p>Dataprep Library, named Dataprep.Clean . By observing and sum-</p>
<p>(1) Split. In the beginning, the data scientist needs to split the</p>
<p>marizing the common steps of data standardization for specific col-</p>
<p>datetime string into several single parts, which include one kind of</p>
<p>umn types, we design unified APIs clean_type(df, column_name,</p>
<p>specific information. In our example, the data scientist can get sev-</p>
<p>target_format) , where the type represents the desired standard-</p>
<p>eral tokens {â€™Thuâ€™,â€™Sepâ€™,â€™25â€™,â€™10â€™,â€™36â€™,â€™28â€™,â€™2003â€™} from</p>
<p>ization type, such as date, address, and phone, etc. These unified</p>
<p>the first record by using space and colon as separators. Each differ-</p>
<p>APIs offer enhanced expressiveness compared to raw Pandas code, reducing the complexity of standardizing specific column types and allowing one to standardize a column with only one line of code.</p>
<p>ent type has its splitting strategy, which may not always be splitting the string into tokens. For example, the data scientist will split the email string into the username part and the domain part.</p>
<p>To solve C2 , we propose the CleanAgent framework which</p>
<p>(2) Validate. Standardization can only be performed on valid</p>
<p>automates data standardization with Dataprep.Clean and LLM-</p>
<p>inputs. Thus, the second step should be validation. For example, if</p>
<p>based Agents [ 5 , 6 ]. Once users have entered their final goals, the</p>
<p>the string â€œlittle catâ€ is an instance of the</p>
<p>LLM-based Agents can free their hands, autonomously generate reasoning steps, and execute particular tasks. Data scientists only</p>
<p>datetime column, this</p>
<p>string is invalid, and the data scientist will transform it to a default value like NaN . Intuitively, a valid string indicates that each part</p>
<p>need to input the table being standardized and their requirements,</p>
<p>of this string after splitting is valid. Usually, the data scientist will</p>
<p>CleanAgent will complete the data standardization process au-tomatically with three steps: annotating the type of each column, generating concise Python code for standardization, and executing</p>
<p>recognize and validate each part by their domain knowledge, some corpus or some rules. If every split part is valid, the string is also valid. For instance, the token â€™Sepâ€™ can be recognized as a valid</p>
<p>the generated Python code.</p>
<p>representation of a month, and â€™2003â€™ can be recognized as a valid</p>
<p>Example 3. Continuing with Example 1. Given an input table ğ‘‡</p>
<p>year.</p>
<p>which needs to be standardized and the data scientistsâ€™ requirements,</p>
<p>(3) Transform. The last step of standardization is to transform</p>
<p>the CleanAgent first recognizes that the â€œAdmission Dateâ€ column belongs to the date type, and the â€œAddressâ€ column belongs to the address type. According to the column-type annotation results, the</p>
<p>each split part and combine them into the target format. In our example, because the target format is</p>
<p>CleanAgent generates and executes Python code for standardization</p>
<p>"YYYY-MM-DD hh:mm:ss" ,</p>
<p>the month Sep is transformed into number 09 and recombined with other parts to the target "2003-09-25 10:36:28" .</p>
<p>by calling the â€œclean_dateâ€ and â€œclean_addressâ€ functions, then returns</p>
<p>â€² The Design of Unified APIs. The goal of our API design is to</p>
<p>the standardized table ğ‘‡ .</p>
<p>enable data scientists to complete all the common steps of stan-</p>
<p>We also built a web interface for CleanAgent . It allows the users to choose sample data and communicate with</p>
<p>dardizing one column with a single function call. Simplicity and</p>
<p>CleanAgent</p>
<p>consistency are considered the principles of API design. The ob-</p>
<p>for standardization. We provide the demonstration video, which</p>
<p>servation of the common steps of data standardization brings the</p>
<p>can be found on Youtube.</p>
<p>type-specific API design idea. More specifically, we design the API</p>
<p>To summarize, we make the following contributions: (1) We</p>
<p>to be in the following form:</p>
<p>propose Dataprep.Clean , an open-sourced library for reducing the complexity of implementing data format standardization with type-</p>
<p>clean_ type (df, column_name, target_format)</p>
<p>specific standardization functions. (2) We propose</p>
<p>CleanAgent ,</p>
<p>which automates the data standardization process by combining</p>
<p>where clean_type is the function name, type represents the</p>
<p>both the advantages of Dataprep.Clean and LLM-based Agents. (3)</p>
<p>type of the current column. The first argument df represents the</p>
<p>We deploy CleanAgent as a web application with a user-friendly</p>
<p>input DataFrame, the second argument</p>
<p>interface and demonstrate its utility. We also open-sourced the</p>
<p>column_name is the column</p>
<p>being standardized, and the third argument</p>
<p>implementation of CleanAgent on Github.</p>
<p>target_format is the</p>
<p>target standardization format users specified. Our API design is flexible and extensible, which makes it convenient for users to add their standardization functions for new data types. Currently, we have 142 standardization functions in</p>
<p>Dataprep.Clean , each</p>
<p>handles one data type. These functions serve to demonstrate the value of a more declarative approach, illustrating that building</p>
<p>Table ğ‘» Standardized</p>
<p>cannot figure out the specific type of one column, the</p>
<p>Column-</p>
<p>+ Userâ€™s Requirements Table ğ‘»</p>
<p>type Annotator outputs â€œI do not knowâ€. The annotation result is</p>
<p>CleanAgent</p>
<p>returned to the Chat Manager and stored in the Chat Manager â€™s</p>
<p>1 Historical 5 Historical</p>
<p>Message Chat Message</p>
<p>memory ( â‘¡ in Figure 2).</p>
<p>Manager</p>
<p>Thirdly, the Python Programmer receives historical messages</p>
<p>2 ğ‘ª : email 6 Success/Error</p>
<p>ğŸ Code</p>
<p>Column - type</p>
<p>ğ‘ª : phone Execution</p>
<p>from the Chat Manager including the column-type annotation re-</p>
<p>ğŸ</p>
<p>Annotator</p>
<p>Executor</p>
<p>4 df = clean_email (</p>
<p>3 Historical</p>
<p>sults ( â‘¢ in Figure 2), picks up the corresponding clean functions,</p>
<p>df, â€œemailâ€)</p>
<p>Tools</p>
<p>Message</p>
<p>and generates Python code for the data standardization process. The</p>
<p>df = clean_date (</p>
<p>df, â€œ dateâ€)</p>
<p>generated Python code is also returned to the</p>
<p>Tools stored in the Chat Manager â€™s memory ( â‘£ in Figure 2). Finally, the</p>
<p>Chat Manager and</p>
<p>Code Executor receives historical messages from the Chat Manager</p>
<p>Until successfully</p>
<p>Python 1 - 6</p>
<p>including the column-type annotation results and the generated</p>
<p>standardize input table ğ‘»</p>
<p>Programmer</p>
<p>Python code ( â‘¤ in Figure 2), then executes the generated Python code. If the generated code executes without errors, the standard-</p>
<p>Figure 2: The Workflow of CleanAgent.</p>
<p>ized table ğ‘‡ is returned; otherwise, the error message is returned to the Chat Manager and stored in its memory ( â‘¥ in Figure 2). Then, CleanAgent will retry the whole workflow until it can complete the data standardization process successfully.</p>
<p>declarative data standardization tools for LLMs is not only feasible but essential, motivating the community to develop even more advanced tools.</p>
<h2>3 CLEANAGENT WORKFLOW</h2>
<h2>4 EXPERIMENTS</h2>
<p>Dataset. In our experiment, we employ the Flights dataset from [ 4 ], as it contains highly irregular datetime formats across four at-tributes: scheduled_dept, actual_dept, scheduled_arrival,</p>
<p>In this section, we first introduce the basic structure of LLM-based agents. Then, we describe the CleanAgent workflow constructed</p>
<p>and actual_arrival . The datetime values in these columns ex-hibit a wide variety of inconsistent formats, such as</p>
<p>"2011-12-08</p>
<p>by four agents. The automatic data standardization process can be completed by the cooperation of the four agents in</p>
<p>3:50:00 PM", "2:30pDec 27", "06:45 AM Sun 25-Dec-2011" ,</p>
<p>CleanAgent .</p>
<p>etc. This makes the dataset particularly suitable for evaluating the</p>
<p>Basic Structure of LLM-based Agents.</p>
<p>According to the previous</p>
<p>standardization capabilities of different systems.</p>
<p>surveys on LLM-based Agent [ 6 ], an LLM-based agent includes four</p>
<p>Baselines. We compare CleanAgent with the following two base-</p>
<p>main components: (1) a backbone LLM used to generate replies for</p>
<p>lines: (1) GPT-4o + Prompting. Data standardization code can be di-</p>
<p>input prompts, (2) a memory used to store historical conversation</p>
<p>rectly generated by prompting powerful chat models such as GPT-</p>
<p>messages, (3) a system message defining the role of the agent, and (4)</p>
<p>4o. (2) Cocoon [8] . Cocoon is a one-shot data cleaning system that</p>
<p>a set of external tools which can be called by the LLM-based agent to complete specific tasks, such as web searching, code execution,</p>
<p>decomposes complex cleaning tasks into manageable components within a workflow designed to mimic human cleaning processes,</p>
<p>etc.</p>
<p>leveraging large language models. It supports a variety of data</p>
<p>Detailed Workflow. The detailed workflow of CleanAgent is</p>
<p>shown in Figure 2. The CleanAgent is composed of four agents, including a Chat Manager , a Column-type Annotator , a Python Pro-grammer , and a Code Executor . They can communicate with each other and automatically complete the data standardization process</p>
<p>cleaning tasks, including missing value imputation, outlier detection, and functional dependency violation. In this paper, however, we focus on evaluating Cocoonâ€™s ability for data standardization. Note that there are other LLM-based data cleaning approaches, such as RetClean [ 2 ] . However, RetClean primarily adopts a retrieval-</p>
<p>by cooperation. Each agent has its own memory to store the his-torical conversational messages between it and other agents. Note that the memory of the Chat Manager is uniquely comprehensive, encompassing the entire historical conversational messages from all agents within the CleanAgent system. This extensive memory</p>
<p>based strategy such as RAG to enhance the ability of LLMs for data cleaning, which supplements the LLM with user-provided data sources. This paradigm is not suitable for our scenario. Ground Truth Generation. We find that GPT-4o can reliably convert individual datetime strings into a target format (e.g., YYYY-</p>
<p>enables every agent in the CleanAgent to generate responses that</p>
<p>MM-DD HH:MM:SS). Thus, we use it to generate cell-level ground</p>
<p>are informed by the complete historical messages.</p>
<p>truth values and compile them into a complete table.</p>
<p>The input of CleanAgent includes a table ğ‘‡ that needs to be standardized. Data scientists can also input extra requirements such as â€œthe format of the date type column should be MM/DD/YYYYâ€.</p>
<p>Metrics. We use the average cell-level matching rate across all columns as our evaluation metric. For a given table ğ‘‡ , the cell-level matching rate is computed as:</p>
<p>By receiving the input table and data scientistsâ€™ extra requirements, CleanAgent stores this information into the Chat Managerâ€™s mem-</p>
<p>Ã Ã</p>
<p>ğ‘š ğ‘›</p>
<p>ory and then completes the data standardization process. The Chat 1 ( ğ‘‡ = ğ‘‡ ) Manager delivers messages in its memory to the Column-type An-</p>
<p>ğ‘ğ‘™ğ‘’ğ‘ğ‘› ğ‘”ğ‘¡</p>
<p>ğ‘– = 1 ğ‘— = 1 ğ‘– ğ‘— ğ‘– ğ‘—</p>
<p>ğ‘‘ ( ğ‘‡ ,ğ‘‡ gt ) = (1)</p>
<p>clean</p>
<p>ğ‘š</p>
<p>notator ( â‘  in Figure 2). Then, The Column-type Annotator receives the table information and leverages an LLM to annotate the type of</p>
<p>where 1 [Â·] is the indicator function, and ğ‘‡ and ğ‘‡ denote the</p>
<p>clean gt</p>
<p>each column in the input table. If the The Column-type Annotator</p>
<p>standardized and ground truth tables, respectively.</p>
<p>1 4 4</p>
<p>2</p>
<p>5</p>
<p>3</p>
<p>6</p>
<p>Figure 3: User interface of CleanAgent.</p>
<p>Implementation. CleanAgent is implemented</p>
<p>Then CleanAgent shows the basic information of the uploaded</p>
<p>1</p>
<p>in Python 3.10.6. Cocoon is run using its official Colab notebook</p>
<p>file (number of rows and number of columns). If the users can click</p>
<p>2</p>
<p>from the GitHub repo . All methods use the gpt-4o-2024-08-06 model. Experiments are conducted on a MacBook Pro with an M1</p>
<p>the â€œStart Standardizationâ€ button to start the data standardization process by want CleanAgent .</p>
<p>chip, 16GB RAM, running macOS Sequoia 15.5.</p>
<p>After clicking the â€œStart Standardizationâ€ button, as area â‘¡</p>
<p>Results. Table 1 presents the comparison of different systems in terms of cell-level matching rate and latency.</p>
<p>shows, the User_Proxy generates three detailed steps to complete</p>
<p>CleanAgent achieves the data standardization task. Firstly, the</p>
<p>a 42.5% cell-level matching rate, approximately 2 Ã— higher than that</p>
<p>Column-type Annotator</p>
<p>receives messages from the Chat Manager , annotates and out-</p>
<p>of GPT-4o and Cocoon. These results demonstrate that Clean-</p>
<p>puts the type of each column, as area â‘¢ shows. Then, the Python</p>
<p>Agent â€™s type-specific standardization API enhances the LLMâ€™s ability to generate more precise and concise standardization code.</p>
<p>Programmer picks up standardization functions from Dataprep.Clean based on the type of each column, and write proper Python code</p>
<p>In addition to higher accuracy, CleanAgent also exhibits lower latency compared to Cocoon. This is because Cocoon generates a one-shot SQL query for all columns without the ability to target specific ones, leading to unnecessary overhead.</p>
<p>using the standardization functions, as area â‘£ shows. Thirdly, the Code Executor executes the Python code by the Python Programmer and collects the execution messages, as area â‘¤ shows. If the Code Executor gets an error message when executing generated Python code, the error message is sent to the</p>
<p>Table 1: Data standardization performance by comparing</p>
<p>Chat Manager and becomes</p>
<p>part of the prompt of the next try. If the</p>
<p>different systems.</p>
<p>Code Executor gets the</p>
<p>message of successful execution,</p>
<p>CleanAgent will report that</p>
<p>the data standardization is completed, as area â‘¥ shows. Moreover,</p>
<p>Cell-Level</p>
<p>users can click the â€œShow Cleaned Tableâ€ button to check whether</p>
<p>System</p>
<p>Latency (s)</p>
<p>Matching Rate(%)</p>
<p>the standardized table matches their requirements. If so, users can</p>
<p>GPT-4o 22.0 19.76</p>
<p>download the standardized table directly. Otherwise, users can input</p>
<p>Cocoon 21.5 636.62</p>
<p>their extra requirements with natural language, and CleanAgent</p>
<p>CleanAgent 42.5 29.57</p>
<p>will start a new data standardization process accordingly.</p>
<h2>5 USER INTERFACE OF CLEANAGENT</h2>
<h2>6 CONCLUSION</h2>
<p>We developed a web-based user interface for</p>
<p>CleanAgent , allow-</p>
<p>In this paper, we proposed CleanAgent to automate the data stan-</p>
<p>ing users to simply upload their tables without performing any operations. The system then automatically returns the standardized</p>
<p>dardization process with Dataprep.Clean and LLM-based Agents. We implemented CleanAgent as a web application to visualize</p>
<p>results of their data.</p>
<p>Figure 3 shows the user interface of CleanAgent . As area â‘  shows, users must first upload a CSV file that needs to be cleaned.</p>
<p>the conversations among agents. Other tasks in the data science life cycle, such as data cleaning and data visualization, can also be completed by LLM-based agents [ 7 ]. In the future, it is promising that the data science life cycle can be automatically planned and</p>
<p>blob/main/demo/Cocoon_Stage_Demo.ipynb</p>
<p>completed by LLM-based agentsâ€™ cooperation.</p>
<h2>REFERENCES</h2>
<p>System Message of Column Annotator</p>
<p>[1] Sibei Chen, Hanbing Liu, Weiting Jin, Xiangyu Sun, Xiaoyao Feng, Ju Fan, Xi-</p>
<p>aoyong Du, and Nan Tang. 2023. ChatPipe: Orchestrating Data Preparation</p>
<p>You are an expert column type annotator.</p>
<p>Program by Optimizing Human-ChatGPT Interactions. CoRR abs/2304.03540</p>
<p>Please solve the column type annotation task following</p>
<p>(2023). https://doi.org/10.48550/ARXIV.2304.03540 arXiv:2304.03540</p>
<p>[2] Mohamed Y. Eltabakh, Zan Ahmad Naeem, Mohammad Shahmeer Ahmad, Mourad</p>
<p>the instruction. Please ALWAYS show the column annota-</p>
<p>Ouzzani, and Nan Tang. 2024. RetClean: Retrieval-Based Tabular Data Cleaning</p>
<p>tion result!!! Please ONLY return the column annotation</p>
<p>Using LLMs and Data Lakes. Proc. VLDB Endow. 17, 12 (2024), 4421â€“4424. https:</p>
<p>result adding a sentence "Please using corresponding clean</p>
<p>//doi.org/10.14778/3685800.3685890</p>
<p>[3] Wes McKinney et al . 2024. pandas: powerful Python data analysis toolkit. https:</p>
<p>functions and write code to clean the column"!!!</p>
<p>//pandas.pydata.org/ Accessed: 2024-01-25.</p>
<p>Classify the columns of a given table with only one of the</p>
<p>[4] Theodoros Rekatsinas, Xu Chu, Ihab F. Ilyas, and Christopher RÃ©. 2017. HoloClean:</p>
<p>Holistic Data Repairs with Probabilistic Inference. Proc. VLDB Endow. 10, 11 (2017),</p>
<p>following classes that are seperated with comma: {candi-</p>
<p>1190â€“1201. https://doi.org/10.14778/3137628.3137631</p>
<p>date_column_types}.</p>
<p>[5] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang</p>
<p>Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023. AutoGen:</p>
<p>(1) Look at the input given to you and make a table</p>
<p>Enabling Next-Gen LLM Applications via Multi-Agent Conversation Frame-</p>
<p>out of it.</p>
<p>work. CoRR abs/2308.08155 (2023). https://doi.org/10.48550/ARXIV.2308.08155</p>
<p>(2) Look at the cell values in detail.</p>
<p>arXiv:2308.08155</p>
<p>[6] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming</p>
<p>(3) For each column, select a class that best represents</p>
<p>Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xi-</p>
<p>the meaning of all cells in the column.</p>
<p>(4) Answer with the selected class for each columns</p>
<p>angyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huan, and Tao Gui. 2023. The Rise and Potential of Large Language Model Based Agents: A</p>
<p>with the format **columnName: class**. If you can-</p>
<p>Survey. CoRR abs/2309.07864 (2023). https://doi.org/10.48550/ARXIV.2309.07864</p>
<p>arXiv:2309.07864</p>
<p>not confidently classify a column based on the pro-</p>
<p>[7] Siqiao Xue, Caigao Jiang, Wenhui Shi, Fangyin Cheng, Keting Chen, Hongjun</p>
<p>vided data, output "I do not know" for that column. NOTE THAT You MUST provide exactly one classification</p>
<p>Yang, Zhiping Zhang, Jianshan He, Hongyang Zhang, Ganglin Wei, et al . 2023. Db-gpt: Empowering database interactions with private large language models.</p>
<p>for EVERY column â€” no column should be left unclassified.</p>
<p>arXiv preprint arXiv:2312.17449 (2023).</p>
<p>Sample rows of the given table is shown as follows: {df}.</p>
<p>[8] Shuo Zhang, Zezhou Huang, and Eugene Wu. 2024. Data Cleaning Using Large</p>
<p>Language Models. CoRR abs/2410.15547 (2024). https://doi.org/10.48550/ARXIV.</p>
<p>2410.15547 arXiv:2410.15547</p>
<p>System Message of Python Code Generator</p>
<p>You are a senior Python engineer who is responsible for writing Python code to clean the input DataFrame. You can use the following libraries: pandas, numpy, re, datetime, dataprep, and any other libraries you want. Note that the Dataprep library takes the first priority. The Dataprep library is used to standardize the data. You can find the documentation of Dataprep library here:</p>
<p><a href="https://sfu-db.github.io/dataprep/">https://sfu-db.github.io/dataprep/</a>.</p>
<p>Please only output the code.</p>
<p>A PROMPTS OF COMPONENT IN</p>
<p>CLEANAGENT</p>
<p>System Message of Chat Manager</p>
<p>Use dataprep library to clean the table {path}. Please follow the three steps:</p>
<p>(1) Use column annotator to annotate the type</p>
<p>of each column within the five types: {candi-</p>
<p>date_column_types}.</p>
<p>(2) Pick up corresponding clean functions and write</p>
<p>System Message of Python Code Executor</p>
<p>code to clean the column.</p>
<p>(3) store the cleaned dataframe as csv file named as</p>
<p>You are a Python code executor that executes the code</p>
<p>"cleaned_data.csv"</p>
<p>written by the engineer and reports the result.</p>
<p>B DETAILED EXPERIMENT SETTINGS B.2 Detailed GPT Settings</p>
<p>For CleanAgent , we use GPT-4o with a temperature of 0, a timeout</p>
<p>B.1 Prompt of GPT-4o Baseline</p>
<p>of 60 seconds, and a cache seed of 42. For Cocoon [ 8 ], we follow the</p>
<p>System Message of Chat Manager</p>
<p>default setting and set the temperature to 1. For the GPT-4o baseline, the temperature is also set to 0 for consistency with CleanAgent</p>
<p>You are an expert data standardizer.</p>
<p>.</p>
<p>Task: Given a CSV file **raw.csv** in the current working directory, do two things:</p>
<p>1. Column typing</p>
<p>Inspect the data and output one best-fit type for each col-umn, line by line in the form:</p>
<p>columnName: class</p>
<p>2. Generate Python script</p>
<p>After a blank line, provide a single Python script (inside</p>
<p>â€œâ€˜python fences) that:</p>
<p>- reads raw.csv</p>
<p>- standardizes every column WITHOUT USING ANY</p>
<p>Python libraries</p>
<p>- no additional explanations.</p>
<p>- please notice the python code, **please not using any li-braries** such as**datetime, parse, colorsys, pandas**. Only the original way and regex can be used.</p>
<p>- if a cell cannot be recognized according to the columnâ€™s target format, return â€˜NaNâ€˜.</p>
<p>- formatting rules for column types:</p>
<p>(1) date â†’ yyyy-mm-dd hh:mm:ss</p>
<p>(2) address â†’ Apt apartment_number, house_number,</p>
<p>street_name, city, state_abbreviation, country, zip-code (skip any missing part silently)</p>
<p>(3) phone_number â†’ E.164 format</p>
<p>(4) location â†’ (lat,lon)</p>
<p>(5) ip â†’ plain IP without subnet mask</p>
<p>(6) url â†’ JSON object with keys:</p>
<p>{</p>
<p>â€™schemeâ€™: â€™httpâ€™,</p>
<p>â€™hostâ€™: â€™www.example.comâ€™,</p>
<p>â€™url_cleanâ€™: â€™http://www.example.com/pathâ€™,</p>
<p>â€™queriesâ€™: {</p>
<p>â€™key1â€™: â€™value1â€™,</p>
<p>â€™key2â€™: â€™value2â€™</p>
<p>}</p>
<p>}</p>
<p>(7) duration â†’ hh:mm:ss</p>
<p>(8) temperatures â†’ Celsius format, e.g., 23 â„ƒ</p>
<p>(9) colors â†’ hexadecimal, e.g., #a1b2c3</p>
<p>(10) names â†’ "firstname lastname" - If format is "last-name, firstname", convert it - If already "firstname lastname", keep it unchanged</p>
<p>Writes cleaned_data.csv in the same directory</p>
<p>The script must be runnable with â€˜python script.pyâ€˜ in a standard Python environment (pandas &amp; common pip packages installed)</p>
<p>Return only the column typing and script. No additional explanations.</p>
<p>Sample rows of the given table is shown as follows: {df}</p>
<p>1 <a href="https://colab.research.google.com/github/Cocoon-Data-Transformation/cocoon/">https://colab.research.google.com/github/Cocoon-Data-Transformation/cocoon/</a></p>
<p>2 <a href="https://cocoon-data-transformation.github.io/page/clean">https://cocoon-data-transformation.github.io/page/clean</a></p>
</body>
</html>
