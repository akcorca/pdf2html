<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Converted PDF</title>
</head>
<body>
<h1>CleanAgent: Automating Data Standardization with LLM-based Agents</h1>
<div class="authors">
Simon Fraser University<br>
{dqi, zhengjie, jnwang}@sfu.ca
</div>
<h2>ABSTRACT</h2>
<p>Data standardization is a crucial part of the data science life cycle. While tools like Pandas offer robust functionalities, their complex-ity and the manual effort required for customizing code to diverse column types pose significant challenges. Although large language models (LLMs) like ChatGPT have shown promise in automating this process through natural language understanding and code gen-eration, it still demands expert-level programming knowledge and continuous interaction for prompt refinement. To solve these chal-lenges, our key idea is to propose a Python library with declarative, unified APIs for standardizing different column types, simplify-ing the LLMâ€™s code generation with concise API calls. We first propose Dataprep.Clean , a component of the Dataprep Python Library, significantly reduces the coding complexity by enabling the standardization of specific column types with a single line of code. Then, we introduce the CleanAgent framework integrat-ing Dataprep.Clean and LLM-based agents to automate the data standardization process. With CleanAgent , data scientists only need to provide their requirements once, allowing for a hands-free</p>
<p>aims to transform heterogeneous data formats within a single col-umn into a unified data format. This crucial data preprocessing step is essential for enabling effective data integration, data analysis, and decision-making.</p>
<p>process.</p>
<h2>1 INTRODUCTION</h2>
<p>Data standardization, which is pivotal in the realm of data science,</p>
<pre><code>1 def standardize_address ( addr ):
2 # Extract street number and street name
3 street = pd . Series ( addr ). str . extract (r ' (\ d+ [^ ,]+) ' ).
squeeze ()
4 # Extract state name
5 state = " LA "
6 # Extract zipcode
7 zipcode = pd . Series ( addr ). str . extract (r ' (\ d {5}) ' ).
squeeze ()
8 # Output standardized address
9 return f"{ street }, { state }, { zipcode }"</code></pre>
<p>Example 1. We illustrate the data standardization task in Figure 1.</p>
<p>Given the input table ğ‘‡ , it is obvious that data in the â€œAdmission Dateâ€ column and the â€œAddressâ€ column are in different formats, and the data in the cells of the â€œAdmission Dateâ€ column includes two different date formats. The goal of data standardization is to unify the data format in each column in ğ‘‡ , to get the standardized table</p>
<p>ğ‘‡ satisfying the data scientistâ€™s requirements. In Figure 1, the data</p>
<p>scientist inputs their requirement to standardize â€œAdmission Dateâ€</p>
<p>with the â€œMM/DD/YYYY HH:MM:SSâ€ format. In the resulting ğ‘‡ , data</p>
<p>format, i.e., the â€œMM/DD/YYYY HH:MM:SSâ€ format.</p>
<p>Previously, data scientists heavily relied on libraries such as Pandas [ 3 ] for data standardization tasks. Even though Pandas is a powerful tool, achieving data standardization often requires writing hundreds or thousands of lines of code. The standardization process for a single column involves identifying the column type, applying intricate methods such as regular expressions to each cell for validation, and converting each cell into desired formats. Moreover, a table may contain multiple columns, each possibly of a different type, requiring bespoke standardization code for each column type.</p>
<p>Example 2. Still considering the data standardization task in Figure 1. For standardizing â€œAdmission Dateâ€ and â€œAddressâ€, data sci-entists need to write the datetime standardization code for â€œAdmission Dateâ€ and address standardization code for â€œAddressâ€ using regex. An example standardization code for â€œAddressâ€ is shown as follows .</p>
<p>If the input table ğ‘‡ has other column types such as email and IP addresses, data scientists also need to write standardization code tailored for the new types, which is time-consuming.</p>
<p>Recently, the emerging LLMs have shown the potential to rev-</p>
<p>in the cells of the â€œAdmission Dateâ€ column follows only one date</p>
<p>olutionize this process. By leveraging their natural language un-derstanding and code generation ability, these models could signif-icantly aid data scientists by autonomously generating standard-ization code in response to conversational prompts. However, this method still necessitates detailed prompt crafting and often in-volves multi-turn dialogues [ 1 ] for different column types in the table one by one, which limits the efficiency and practicality of adopting LLMs in the standardization process.</p>
<p>To overcome these limitations, our key idea is to introduce a Python library involving declarative and unified APIs specifically designed for standardizing different column types. This idea lowers the burden of the LLM, as it now only needs to convert natural lan-guage (NL) instructions into succinct, declarative API calls instead of lengthy, procedural code. Such an approach simplifies the LLMâ€™s</p>
<p>code generation process for data standardization, requiring just a few lines of code.</p>
<h2>2 TYPE-SPECIFIC STANDARDIZATION API DESIGN</h2>
<p>The pursuit of simplicity, however, introduces two primary chal-lenges. The first challenge ( C1 ) is the design of the declarative and unified APIs for data standardization, ensuring it can effectively re-duce the intricacies involved in standardizing specific column types (ideally one line of code per column type). The second challenge ( C2 ) centers on optimizing the interaction between data scientists and LLMs. Our goal is to minimize human involvement, ideally allowing data scientists to input their standardization requirements in one instance, thereby enabling an autonomous and hands-off data standardization process.</p>
<p>To solve C1 , we propose the type-specific Clean module in the Dataprep Library, named Dataprep.Clean . By observing and sum-marizing the common steps of data standardization for specific col-umn types, we design unified APIs clean_type(df, column_name, target_format) , where the type represents the desired standard-ization type, such as date, address, and phone, etc. These unified APIs offer enhanced expressiveness compared to raw Pandas code, reducing the complexity of standardizing specific column types and allowing one to standardize a column with only one line of code. To solve C2 , we propose the CleanAgent framework which automates data standardization with Dataprep.Clean and LLM-based Agents [ 5 , 6 ]. Once users have entered their final goals, the LLM-based Agents can free their hands, autonomously generate reasoning steps, and execute particular tasks. Data scientists only need to input the table being standardized and their requirements,</p>
<p>value like NaN . Intuitively, a valid string indicates that each part</p>
<p>CleanAgent will complete the data standardization process au-tomatically with three steps: annotating the type of each column, generating concise Python code for standardization, and executing</p>
<p>valid. For instance, the token â€™Sepâ€™ can be recognized as a valid</p>
<p>the generated Python code.</p>
<p>Example 3. Continuing with Example 1. Given an input table ğ‘‡ which needs to be standardized and the data scientistsâ€™ requirements, the CleanAgent first recognizes that the â€œAdmission Dateâ€ column belongs to the date type, and the â€œAddressâ€ column belongs to the address type. According to the column-type annotation results, the CleanAgent generates and executes Python code for standardization by calling the â€œclean_dateâ€ and â€œclean_addressâ€ functions, then returns</p>
<p>â€² The Design of Unified APIs. The goal of our API design is to</p>
<p>the standardized table ğ‘‡ .</p>
<p>We also built a web interface for CleanAgent . It allows the users to choose sample data and communicate with</p>
<p>CleanAgent</p>
<p>for standardization. We provide the demonstration video, which can be found on Youtube.</p>
<p>To summarize, we make the following contributions: (1) We propose Dataprep.Clean , an open-sourced library for reducing the complexity of implementing data format standardization with type-specific standardization functions. (2) We propose</p>
<p>CleanAgent ,</p>
<p>which automates the data standardization process by combining both the advantages of Dataprep.Clean and LLM-based Agents. (3) We deploy CleanAgent as a web application with a user-friendly interface and demonstrate its utility. We also open-sourced the implementation of CleanAgent on Github.</p>
<p>we have 142 standardization functions in</p>
<p>value of a more declarative approach, illustrating that building</p>
<p>In this section, we first describe the common steps of data stan-dardization. Then, we introduce the type-specific API design of Dataprep.Clean .</p>
<p>Common Steps of Data Standardization. Inspired by the steps of how human users standardize data cells, we identify three common steps of data standardization. We take the datetime column type as an example to illustrate these steps.</p>
<p>Assume a data scientist is dealing with an datetime column</p>
<p>including two records "Thu Sep 25 10:36:28 2003" and "1996.07.10 AD at 15:08:56" . The data scientist wants to unify the messy column into a target format "YYYY-MM-DD hh:mm:ss" .</p>
<p>(1) Split. In the beginning, the data scientist needs to split the datetime string into several single parts, which include one kind of specific information. In our example, the data scientist can get sev-eral tokens {â€™Thuâ€™,â€™Sepâ€™,â€™25â€™,â€™10â€™,â€™36â€™,â€™28â€™,â€™2003â€™} from the first record by using space and colon as separators. Each differ-ent type has its splitting strategy, which may not always be splitting the string into tokens. For example, the data scientist will split the email string into the username part and the domain part.</p>
<p>(2) Validate. Standardization can only be performed on valid inputs. Thus, the second step should be validation. For example, if the string â€œlittle catâ€ is an instance of the datetime column, this string is invalid, and the data scientist will transform it to a default</p>
<p>of this string after splitting is valid. Usually, the data scientist will recognize and validate each part by their domain knowledge, some corpus or some rules. If every split part is valid, the string is also</p>
<p>representation of a month, and â€™2003â€™ can be recognized as a valid year.</p>
<p>(3) Transform. The last step of standardization is to transform each split part and combine them into the target format. In our example, because the target format is "YYYY-MM-DD hh:mm:ss" , the month Sep is transformed into number 09 and recombined with other parts to the target "2003-09-25 10:36:28" .</p>
<p>enable data scientists to complete all the common steps of stan-dardizing one column with a single function call. Simplicity and consistency are considered the principles of API design. The ob-servation of the common steps of data standardization brings the type-specific API design idea. More specifically, we design the API to be in the following form: clean_ type (df, column_name, target_format) where clean_type is the function name, type represents the type of the current column. The first argument df represents the input DataFrame, the second argument column_name is the column being standardized, and the third argument target_format is the target standardization format users specified. Our API design is flexible and extensible, which makes it convenient for users to add their standardization functions for new data types. Currently, Dataprep.Clean , each handles one data type. These functions serve to demonstrate the</p>
<p>Table ğ‘» Standardized</p>
<p>cannot figure out the specific type of one column, the Column-</p>
<p>+ Userâ€™s Requirements Table ğ‘»</p>
<p>CleanAgent</p>
<p>1 Historical 5 Historical Message Chat Message</p>
<p>Column - type</p>
<p>ğ‘ª : phone Execution</p>
<p>Annotator</p>
<p>4 df = clean_email (</p>
<p>memory ( â‘¡ in Figure 2).</p>
<p>Manager</p>
<p>2 ğ‘ª : email 6 Success/Error</p>
<p>ğŸ Code</p>
<p>standardize input table ğ‘»</p>
<p>from the Chat Manager including the column-type annotation re-</p>
<p>ğŸ</p>
<p>Executor</p>
<p>3 Historical</p>
<p>df, â€œemailâ€)</p>
<p>Tools</p>
<p>Message</p>
<p>df = clean_date (</p>
<p>Programmer</p>
<p>and generates Python code for the data standardization process. The</p>
<p>df, â€œ dateâ€)</p>
<p>Until successfully</p>
<p>4o. (2) Cocoon [8] . Cocoon is a one-shot data cleaning system that</p>
<p>within a workflow designed to mimic human cleaning processes,</p>
<p>type Annotator outputs â€œI do not knowâ€. The annotation result is returned to the Chat Manager and stored in the Chat Manager â€™s</p>
<p>Thirdly, the Python Programmer receives historical messages</p>
<p>sults ( â‘¢ in Figure 2), picks up the corresponding clean functions,</p>
<p>generated Python code is also returned to the Chat Manager and</p>
<p>Code Executor receives historical messages from the Chat Manager including the column-type annotation results and the generated Python code ( â‘¤ in Figure 2), then executes the generated Python code. If the generated code executes without errors, the standard-</p>
<p>Figure 2: The Workflow of CleanAgent.</p>
<p>declarative data standardization tools for LLMs is not only feasible but essential, motivating the community to develop even more advanced tools.</p>
<p>Tools stored in the Chat Manager â€™s memory ( â‘£ in Figure 2). Finally, the</p>
<p>Python 1 - 6</p>
<h2>3 CLEANAGENT WORKFLOW</h2>
<p>In this section, we first introduce the basic structure of LLM-based agents. Then, we describe the CleanAgent workflow constructed by four agents. The automatic data standardization process can be completed by the cooperation of the four agents in</p>
<p>CleanAgent .</p>
<p>Basic Structure of LLM-based Agents.</p>
<p>According to the previous</p>
<p>surveys on LLM-based Agent [ 6 ], an LLM-based agent includes four main components: (1) a backbone LLM used to generate replies for input prompts, (2) a memory used to store historical conversation messages, (3) a system message defining the role of the agent, and (4) a set of external tools which can be called by the LLM-based agent to complete specific tasks, such as web searching, code execution, etc.</p>
<p>Detailed Workflow. The detailed workflow of CleanAgent is</p>
<p>shown in Figure 2. The CleanAgent is composed of four agents, including a Chat Manager , a Column-type Annotator , a Python Pro-grammer , and a Code Executor . They can communicate with each other and automatically complete the data standardization process by cooperation. Each agent has its own memory to store the his-torical conversational messages between it and other agents. Note that the memory of the Chat Manager is uniquely comprehensive, encompassing the entire historical conversational messages from all agents within the CleanAgent system. This extensive memory enables every agent in the CleanAgent to generate responses that are informed by the complete historical messages.</p>
<p>The input of CleanAgent includes a table ğ‘‡ that needs to be standardized. Data scientists can also input extra requirements such as â€œthe format of the date type column should be MM/DD/YYYYâ€. By receiving the input table and data scientistsâ€™ extra requirements, CleanAgent stores this information into the Chat Managerâ€™s mem-ory and then completes the data standardization process. The Chat 1 ( ğ‘‡ = ğ‘‡ ) Manager delivers messages in its memory to the Column-type An-</p>
<p>ğ‘‘ ( ğ‘‡ ,ğ‘‡ gt ) = (1)</p>
<p>notator ( â‘  in Figure 2). Then, The Column-type Annotator receives the table information and leverages an LLM to annotate the type of</p>
<p>where 1 [Â·] is the indicator function, and ğ‘‡ and ğ‘‡ denote the</p>
<p>each column in the input table. If the The Column-type Annotator</p>
<p>ized table ğ‘‡ is returned; otherwise, the error message is returned to the Chat Manager and stored in its memory ( â‘¥ in Figure 2). Then, CleanAgent will retry the whole workflow until it can complete the data standardization process successfully.</p>
<h2>4 EXPERIMENTS</h2>
<p>we focus on evaluating Cocoonâ€™s ability for data standardization.</p>
<p>Dataset. In our experiment, we employ the Flights dataset from [ 4 ], as it contains highly irregular datetime formats across four at-tributes: scheduled_dept, actual_dept, scheduled_arrival, and actual_arrival . The datetime values in these columns ex-hibit a wide variety of inconsistent formats, such as "2011-12-08 3:50:00 PM", "2:30pDec 27", "06:45 AM Sun 25-Dec-2011" , etc. This makes the dataset particularly suitable for evaluating the standardization capabilities of different systems.</p>
<p>Baselines. We compare CleanAgent with the following two base-lines: (1) GPT-4o + Prompting. Data standardization code can be di-rectly generated by prompting powerful chat models such as GPT-</p>
<p>decomposes complex cleaning tasks into manageable components</p>
<p>leveraging large language models. It supports a variety of data cleaning tasks, including missing value imputation, outlier detection, and functional dependency violation. In this paper, however,</p>
<p>Note that there are other LLM-based data cleaning approaches, such as RetClean [ 2 ] . However, RetClean primarily adopts a retrieval-based strategy such as RAG to enhance the ability of LLMs for data cleaning, which supplements the LLM with user-provided data sources. This paradigm is not suitable for our scenario. Ground Truth Generation. We find that GPT-4o can reliably convert individual datetime strings into a target format (e.g., YYYY-MM-DD HH:MM:SS). Thus, we use it to generate cell-level ground truth values and compile them into a complete table.</p>
<p>Metrics. We use the average cell-level matching rate across all columns as our evaluation metric. For a given table ğ‘‡ , the cell-level matching rate is computed as:</p>
<p>Ã Ã</p>
<p>ğ‘š ğ‘›</p>
<p>ğ‘ğ‘™ğ‘’ğ‘ğ‘› ğ‘”ğ‘¡</p>
<p>ğ‘– = 1 ğ‘— = 1 ğ‘– ğ‘— ğ‘– ğ‘—</p>
<p>clean</p>
<p>ğ‘š</p>
<p>clean gt</p>
<p>standardized and ground truth tables, respectively.</p>
<p>1 4 4</p>
<p>2</p>
<p>5</p>
<p>3</p>
<p>6</p>
<p>Figure 3: User interface of CleanAgent.</p>
<p>Implementation. CleanAgent is implemented</p>
<p>Then CleanAgent shows the basic information of the uploaded</p>
<p>1</p>
<p>in Python 3.10.6. Cocoon is run using its official Colab notebook 2 from the GitHub repo . All methods use the gpt-4o-2024-08-06 model. Experiments are conducted on a MacBook Pro with an M1 chip, 16GB RAM, running macOS Sequoia 15.5.</p>
<p>Results. Table 1 presents the comparison of different systems in terms of cell-level matching rate and latency.</p>
<p>CleanAgent achieves the data standardization task. Firstly, the</p>
<p>a 42.5% cell-level matching rate, approximately 2 Ã— higher than that of GPT-4o and Cocoon. These results demonstrate that Clean-Agent â€™s type-specific standardization API enhances the LLMâ€™s ability to generate more precise and concise standardization code. In addition to higher accuracy, CleanAgent also exhibits lower latency compared to Cocoon. This is because Cocoon generates a one-shot SQL query for all columns without the ability to target specific ones, leading to unnecessary overhead.</p>
<p>Table 1: Data standardization performance by comparing</p>
<p>different systems.</p>
<p>Matching Rate(%)</p>
<p>GPT-4o 22.0 19.76</p>
<p>Cocoon 21.5 636.62</p>
<p>CleanAgent 42.5 29.57</p>
<p>will start a new data standardization process accordingly.</p>
<p>We developed a web-based user interface for</p>
<p>CleanAgent , allow-</p>
<p>ing users to simply upload their tables without performing any operations. The system then automatically returns the standardized</p>
<p>We implemented CleanAgent as a web application to visualize</p>
<p>results of their data.</p>
<p>Figure 3 shows the user interface of CleanAgent . As area â‘  shows, users must first upload a CSV file that needs to be cleaned.</p>
<p>blob/main/demo/Cocoon_Stage_Demo.ipynb</p>
<p>file (number of rows and number of columns). If the users can click the â€œStart Standardizationâ€ button to start the data standardization process by want CleanAgent .</p>
<p>After clicking the â€œStart Standardizationâ€ button, as area â‘¡ shows, the User_Proxy generates three detailed steps to complete Column-type Annotator receives messages from the Chat Manager , annotates and out-puts the type of each column, as area â‘¢ shows. Then, the Python Programmer picks up standardization functions from Dataprep.Clean based on the type of each column, and write proper Python code using the standardization functions, as area â‘£ shows. Thirdly, the Code Executor executes the Python code by the Python Programmer and collects the execution messages, as area â‘¤ shows. If the Code Executor gets an error message when executing generated Python code, the error message is sent to the Chat Manager and becomes part of the prompt of the next try. If the Code Executor gets the message of successful execution, CleanAgent will report that the data standardization is completed, as area â‘¥ shows. Moreover,</p>
<p>Cell-Level</p>
<p>users can click the â€œShow Cleaned Tableâ€ button to check whether</p>
<p>System</p>
<p>Latency (s)</p>
<p>the standardized table matches their requirements. If so, users can download the standardized table directly. Otherwise, users can input their extra requirements with natural language, and CleanAgent</p>
<h2>5 USER INTERFACE OF CLEANAGENT</h2>
<h2>6 CONCLUSION</h2>
<p>In this paper, we proposed CleanAgent to automate the data stan-dardization process with Dataprep.Clean and LLM-based Agents.</p>
<p>the conversations among agents. Other tasks in the data science life cycle, such as data cleaning and data visualization, can also be completed by LLM-based agents [ 7 ]. In the future, it is promising that the data science life cycle can be automatically planned and completed by LLM-based agentsâ€™ cooperation.</p>
<h2>REFERENCES</h2>
<p>System Message of Column Annotator</p>
<ol>
<li>[1] Sibei Chen, Hanbing Liu, Weiting Jin, Xiangyu Sun, Xiaoyao Feng, Ju Fan, Xi-aoyong Du, and Nan Tang. 2023. ChatPipe: Orchestrating Data Preparation Program by Optimizing Human-ChatGPT Interactions. CoRR abs/2304.03540 (2023). https://doi.org/10.48550/ARXIV.2304.03540 arXiv:2304.03540</li>
<li>[2] Mohamed Y. Eltabakh, Zan Ahmad Naeem, Mohammad Shahmeer Ahmad, Mourad Ouzzani, and Nan Tang. 2024. RetClean: Retrieval-Based Tabular Data Cleaning Using LLMs and Data Lakes. Proc. VLDB Endow. 17, 12 (2024), 4421â€“4424. https: //doi.org/10.14778/3685800.3685890</li>
<li>[3] Wes McKinney et al . 2024. pandas: powerful Python data analysis toolkit. https: //pandas.pydata.org/ Accessed: 2024-01-25.</li>
<li>[4] Theodoros Rekatsinas, Xu Chu, Ihab F. Ilyas, and Christopher RÃ©. 2017. HoloClean: Holistic Data Repairs with Probabilistic Inference. Proc. VLDB Endow. 10, 11 (2017), 1190â€“1201. https://doi.org/10.14778/3137628.3137631</li>
<li>[5] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023. AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Frame-arXiv:2308.08155 arXiv:2309.07864</li>
<li>[7] Siqiao Xue, Caigao Jiang, Wenhui Shi, Fangyin Cheng, Keting Chen, Hongjun Yang, Zhiping Zhang, Jianshan He, Hongyang Zhang, Ganglin Wei, et al . 2023. arXiv preprint arXiv:2312.17449 (2023).</li>
<li>[8] Shuo Zhang, Zezhou Huang, and Eugene Wu. 2024. Data Cleaning Using Large 2410.15547 arXiv:2410.15547</li>
</ol>
<p>CLEANAGENT</p>
<p>Use dataprep library to clean the table {path}.</p>
<p>(1) Use column annotator to annotate the type</p>
<p>date_column_types}.</p>
<p>(1) Look at the input given to you and make a table</p>
<p>work. CoRR abs/2308.08155 (2023). https://doi.org/10.48550/ARXIV.2308.08155</p>
<p>[6] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xi-angyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huan, and Tao Gui. 2023. The Rise and Potential of Large Language Model Based Agents: A</p>
<p>Db-gpt: Empowering database interactions with private large language models.</p>
<p>A PROMPTS OF COMPONENT IN</p>
<p>(3) For each column, select a class that best represents</p>
<p>Survey. CoRR abs/2309.07864 (2023). https://doi.org/10.48550/ARXIV.2309.07864</p>
<p>System Message of Chat Manager</p>
<p>Please follow the three steps:</p>
<p>of each column within the five types: {candi-</p>
<p>(2) Pick up corresponding clean functions and write code to clean the column.</p>
<p>(3) store the cleaned dataframe as csv file named as "cleaned_data.csv"</p>
<p>You are an expert column type annotator.</p>
<p>Please solve the column type annotation task following the instruction. Please ALWAYS show the column annotation result!!! Please ONLY return the column annotation result adding a sentence "Please using corresponding clean functions and write code to clean the column"!!! Classify the columns of a given table with only one of the</p>
<p>date_column_types}.</p>
<p>out of it.</p>
<p>(2) Look at the cell values in detail.</p>
<p>the meaning of all cells in the column.</p>
<p>(4) Answer with the selected class for each columns with the format **columnName: class**. If you can-not confidently classify a column based on the pro-vided data, output "I do not know" for that column. NOTE THAT You MUST provide exactly one classification for EVERY column â€” no column should be left unclassified. Sample rows of the given table is shown as follows: {df}.</p>
<p>Language Models. CoRR abs/2410.15547 (2024). https://doi.org/10.48550/ARXIV.</p>
<p>System Message of Python Code Generator</p>
<p>You are a senior Python engineer who is responsible for writing Python code to clean the input DataFrame. You can use the following libraries: pandas, numpy, re, datetime, dataprep, and any other libraries you want. Note that the Dataprep library takes the first priority. The Dataprep library is used to standardize the data. You can find the documentation of Dataprep library here:</p>
<p><a href="https://sfu-db.github.io/dataprep/">https://sfu-db.github.io/dataprep/</a>.</p>
<p>Please only output the code.</p>
<p>System Message of Python Code Executor</p>
<p>You are a Python code executor that executes the code written by the engineer and reports the result.</p>
<p>B DETAILED EXPERIMENT SETTINGS B.2 Detailed GPT Settings</p>
<p>For CleanAgent , we use GPT-4o with a temperature of 0, a timeout</p>
<p>B.1 Prompt of GPT-4o Baseline</p>
<p>of 60 seconds, and a cache seed of 42. For Cocoon [ 8 ], we follow the</p>
<p>System Message of Chat Manager</p>
<p>default setting and set the temperature to 1. For the GPT-4o baseline, the temperature is also set to 0 for consistency with CleanAgent</p>
<p>You are an expert data standardizer.</p>
<p>Task: Given a CSV file **raw.csv** in the current working directory, do two things: 1. Column typing Inspect the data and output one best-fit type for each col-umn, line by line in the form: columnName: class 2. Generate Python script After a blank line, provide a single Python script (inside</p>
<p>â€œâ€˜python fences) that:</p>
<p>- reads raw.csv</p>
<p>- standardizes every column WITHOUT USING ANY</p>
<p>Python libraries</p>
<p>- no additional explanations.</p>
<p>- please notice the python code, **please not using any li-braries** such as**datetime, parse, colorsys, pandas**. Only the original way and regex can be used.</p>
<p>- if a cell cannot be recognized according to the columnâ€™s target format, return â€˜NaNâ€˜.</p>
<p>- formatting rules for column types:</p>
<p>(1) date â†’ yyyy-mm-dd hh:mm:ss</p>
<p>(2) address â†’ Apt apartment_number, house_number,</p>
<p>street_name, city, state_abbreviation, country, zip-code (skip any missing part silently) (3) phone_number â†’ E.164 format (4) location â†’ (lat,lon) (5) ip â†’ plain IP without subnet mask (6) url â†’ JSON object with keys:</p>
<p>{</p>
<p>â€™schemeâ€™: â€™httpâ€™,</p>
<p>â€™hostâ€™: â€™www.example.comâ€™,</p>
<p>â€™url_cleanâ€™: â€™http://www.example.com/pathâ€™,</p>
<p>â€™queriesâ€™: {</p>
<p>â€™key1â€™: â€™value1â€™,</p>
<p>â€™key2â€™: â€™value2â€™</p>
<p>}</p>
<p>}</p>
<p>(7) duration â†’ hh:mm:ss</p>
<p>(8) temperatures â†’ Celsius format, e.g., 23 â„ƒ</p>
<p>(9) colors â†’ hexadecimal, e.g., #a1b2c3</p>
<p>(10) names â†’ "firstname lastname" - If format is "last-name, firstname", convert it - If already "firstname lastname", keep it unchanged Writes cleaned_data.csv in the same directory The script must be runnable with â€˜python script.pyâ€˜ in a standard Python environment (pandas &amp; common pip packages installed) Return only the column typing and script. No additional explanations.</p>
<p>Sample rows of the given table is shown as follows: {df}</p>
<p>1 <a href="https://colab.research.google.com/github/Cocoon-Data-Transformation/cocoon/">https://colab.research.google.com/github/Cocoon-Data-Transformation/cocoon/</a></p>
<p>2 <a href="https://cocoon-data-transformation.github.io/page/clean">https://cocoon-data-transformation.github.io/page/clean</a></p>
</body>
</html>
